# <center>WORD COUNT USING SCALA PROGRAMMING WITH APACHE SPARK</center>

Framework Used : [Apache Spark](https://en.wikipedia.org/wiki/Apache_Spark)

Programming Language Used : [Scala](http://www.scala-lang.org/what-is-scala.html)

Executed on : [Hadoop Big Data Platform](https://en.wikipedia.org/wiki/Apache_Hadoop)

## OVERVIEW

This learning lab can be used as a guide to get a high level understanding of Apache Spark with scala programming language. It describes how to write, compile, and run a simple Spark word count application in Scala programming language. We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## Prerequisites
1.	Install Chrome Browser.
2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology
3.	Basic understanding of Apache Hadoop and Big Data.

## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform)

2. To get familiarized with Scala programming.

3. To get familiarized with the RDD operations on data using spark.

## TERMINOLOGIES USED

### Apache Spark - An Introduction 

Apache Spark is an open source cluster computing framework. Spark is advertised as “lightning fast cluster computing”. It has a thriving open-source community and is the most active Apache project at the moment. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. 

It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs. MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark provides a faster and more general data processing platform.

###### Key Features

1. Currently provides APIs in Scala, Java, and Python, with support for other languages (such as R) on the way
2. Integrates well with the Hadoop ecosystem and data sources (HDFS, Amazon S3, Hive, HBase, Cassandra, etc.)
3. Can run on clusters managed by Hadoop YARN or Apache Mesos, and can also run standalone

###### How to Use Apache Spark? 

###### Example : Using spark to detect an earthquake by analyzing the twitter stream

1. Using Spark streaming, filter tweets that seem relevant like “earthquake” or “shaking”. 

2. Run semantic analysis on the tweets to determine if they appear to be referencing a current earthquake occurrence. Tweets like ”Earthquake!” or ”Now it is shaking”, for example, would be considered positive matches, whereas tweets like “Attending an Earthquake Conference” or ”The earthquake yesterday was scary” would not. 

3. Using the streaming technique we could detects the  positive tweets in a defined time window and thereby can be used to send alert messages.

For more details, please refer:
http://spark.apache.org/
https://en.wikipedia.org/wiki/Apache_Spark


### Scala - An Introduction ###

Scala is an acronym for “Scalable Language”. This means that Scala grows with you. Scala could be written by typing one-line expressions and observing the results and  could also be used for large mission critical systems. Scala could also be considered as a scripting language and is a pure-bred object-oriented language. The language supports advanced component architectures through classes and traits. Even though its syntax is fairly conventional, Scala is also a full-blown functional language.Scala runs on the JVM. Java and Scala classes can be freely mixed, no matter whether they reside in different projects or in the same. Scala makes deliver things faster with less code.

For more details, please refer:
http://www.scala-lang.org/what-is-scala.html
https://en.wikipedia.org/wiki/Scala_(programming_language)


### Hadoop - An Introduction ###

Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. To understand Hadoop, there are two fundamental things about it -  How Hadoop stores files and how it processes data.The framework that is used in hadoop to process data is called MapReduce.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.

Example : Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.

For more details, please refer:
(https://en.wikipedia.org/wiki/Apache_Hadoop)

### DLP - An Introduction ###

The DevNet Data Learning Platform (DLP) is an integrated data platform from CISCO that includes an easy-to-use UI, Docker-     based infrastructure, best-in-class open-source big-data components, and Cisco’s APIs and tools for data developers and data   scientists who want to develop, validate and provision their solutions before deploying or to explore, analyze, and           visualize their data. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built     on Hadoop.


## USE CASE ##

WordCount is used as an example here to demonstrate the use of Scala programming language using Apache Spark on Hadoop. The purpose of this program is to count how many times a word occurs in a text using RDD. The scala program used does the following:

1. Create a [SparkContext](http://spark.apache.org/docs/1.5.0/api/scala/index.html#org.apache.spark.SparkContext). 
2. Load input data from a file.
3. Read an input set of text and split the data in to words.
4. Counts the number of times each word appears and transform it in to word and count.
5. Save the word count output back to a text file.

## PROCESS OVERVIEW 

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/process.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the word count program.

###### Step 1: 

Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology by giving the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

Step 2:

Step 3:

Step 4:

Step 5:

Step 6:

Step 7:


Step 8:

Step 9:

Step 10:

1)	After logging on DLP, click on <b>Development Hub</b> on the left column.<br>
2)	Select the pre-defined work space named as <b>wksp-wordcount</b>.<br>
3)	Click on the <b>Launch</b> button to open IDE workspace.<br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/SelectWorkSpace.PNG?raw=true)

## Step 2: Explore IDE Workspace
For writing any programming language, DLP platform is pre-loaded with an IDE. User can write Spark programming language in this IDE. </br>
Files are listed in Left Panel of this IDE. Double-clicking on any file, right panel will be populated with the content of that specified file. User can directly edit the content from there. </br>
*Below files are required for this learning lab. Except for these files, other files should be used for another learning lab which are not related to this learning lab. </br>
WordCount.scala, WordCountRun.sh, wordcountinputfiles2.txt.* </br>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/WordCountScalafile.PNG?raw=true)


``` json
//Import libraries which are needed to run the program. 
import org.apache.spark.{SparkContext, SparkConf}
object WordCount
{
  def main(args: Array[String]) {
    val inputFile = args(0)
    val outputFile = args(1)
    val conf = new SparkConf().setAppName("wordCount")
    // Create a Scala Spark Context.
    val sc = new SparkContext(conf)
    // Load our input data.
    val input = sc.textFile(inputFile)
    // Split up into words.
    val words = input.flatMap(line => line.split(" "))
    // Transform into word and count.
    val counts = words.map(word => (word, 1)).reduceByKey{_ + _}
    // Save the word count back out to a text file, causing evaluation.
    counts.saveAsTextFile(outputFile)
  }
}
```
N.B: Use WordCount.scala to view the above code. 
In the above, Scala code using Spark is using to read the input file (passed as a parameter), generate the Spark context, Read the input file and split words using a single space and increment the counter of each occurance of word and write the output in console.

**Edit input file name with a sentence.** </br>
Double click on wordcountinputfiles2.txt from the left panel. This file name may differ for differenet user. Edit the file by adding some content there.
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/EditInputFile.png?raw=true)

## Step 3: Build and Execute Spark Program from DLP's IDE 

In this step, User will learn the process of packaging the program from DLP's Cloud IDE. User can use the sample word count program to package and build. This process will work on DLP's Hadoop environment.
1) Double click on WordCound.scala file again. Change <b>package</b> as described in the below image and click on the <b>run</b> blue button. This process will build and package the program.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildWordCount.PNG?raw=true)

2) You can find the below screen if build process finished successfully.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/buildSuccessWordCount.PNG?raw=true)

3) If the build process finished successfully, then select <b>run</b> as described in the below image and click on <b>blue run</b> button again. 
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/runWordCount.PNG?raw=true)

4) The Successful execution of run process will show the output as below. (There should NOT be any Exception message in the exception window if run process finished successfully.)
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/successfulBuildProcess.png?raw=true)

## Step 4: Displaying output from DLP's IDE. 
Below steps are describing the process of generating the displaying the output. 

1) After completing above steps, open the <b>view.sh</b> by double clicking on it. Select <b>view</b> and click on the <b>blue run</b> button to view the output.

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/viewOutput.png?raw=true)



This will show the number of occurance of each word in the output window. <b>view.sh</b> file is used to pickup the file from Hadoop environment and show the output in IDE's output window. 


Here Input file has the text: <b>Hello spark I love Cisco Company. Hello all. We are practising Word Count Program using Scala and Spark.</b>
Output content is as below:
```json
(are,1)
(Program,1)
(Hello,2)
(love,1)
(Word,1)
(practising,1)
(using,1)
(We,1)
(Scala,1)
(spark,1)
(Count,1)
(I,1)
(Company.,1)
(Spark.,1)
(Cisco,1)
(and,1)
(all.,1)
```

Things to Try:

* Try with numeric data type
* Try with case sensitive data as well.

Completing this coding exercise, we have learned how to count the number of words in an input file using Spark Batch Processing. <br>

## REFERENCES

https://en.wikipedia.org/wiki/Apache_Spark
http://www.scala-lang.org/what-is-scala.html
https://en.wikipedia.org/wiki/Apache_Hadoop
https://www.toptal.com/spark/introduction-to-apache-spark


There are few more examples and exercises are available in the below-mentioned link. This is for your reference.
[http://spark.apache.org/docs/latest/programming-guide.html](http://spark.apache.org/docs/latest/programming-guide.html).
