# <center>DEVELOP A STREAM HANDLER FOR CISCO CSA </br>(CONNECTED STREAMING ANALYTICS)</center>

Stream Processing Platform Used : Apache Kafka

Programming Language Used : Java, XML

Executed on : Hadoop Big Data Platform

Visualization tool Used : Apache Zeppelin


## OVERVIEW

This learning lab can be used as a guide to get a high level understanding for using CSA(Connected Streaming Analytics) in  DLP(DevNet Data Learning Platform) platform. DLP platform has Kafka messaging framework that works from the backend. 

![Figure](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/csa.jpg?raw=true)

## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology (Detailed instructions given below)

3.	Basic understanding of Apache Hadoop and Big Data.

3.	Basic knowledge of Java, SQL and XML.


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform)

2. To get familiarized with building custom handler for CSA in DLP and analyze the data at real time. 

## TERMINOLOGIES USED

## Cisco® CONNECTED STREAMING ANALYTICS(CSA) - AN INTRODUCTION

Cisco® Connected Streaming Analytics (CSA) is an analytics platform that delivers predictive, actionable insights from high-velocity streams of live data.

###### Key Features:

1. Streaming query processing supports active, continuous monitoring of live data. This provides instantaneous, real-time analysis and action, as well as efficient use of computing resources. 

2. CSA’s framework and interfaces are ideal for use case development across a wide variety of business and network management functions and industries. 

3. CSA provides real-time insights into big data views to support actionable events and dynamic dashboards to help you get more value out of your data.

###### Use Case:


For more details, please refer:</br>
http://www.cisco.com/c/dam/en/us/products/collateral/analytics-automation-software/streaming-analytics/connected-streaming-analytics-aag.pdf

http://www.cisco.com/c/dam/en/us/products/collateral/analytics-automation-software/streaming-analytics/connected-streaming-analytics.pdf

## APACHE KAFKA? - AN INTRODUCTION

Kafka is a distributed streaming platform that is designed to be fast, scalable, and durable. It has 3 key capabilities:

1.	It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.
2.	It lets you store streams of records in a fault-tolerant way.
3.	It lets you process streams of records as they occur.

It gets used for two broad classes of application:

1.	Building real-time streaming data pipelines that reliably get data between systems or applications
2.	Building real-time streaming applications that transform or react to the streams of data

For more details, please refer:
https://kafka.apache.org/


### HADOOP - AN INTRODUCTION ###

Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. To understand Hadoop, there are two fundamental things about it -  How Hadoop stores files and how it processes data.The framework that is used in hadoop to process data is called MapReduce.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.

Example : Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.

For more details, please refer:
(https://en.wikipedia.org/wiki/Apache_Hadoop)


### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is a Big-Data learning platform connected with an easy to learn UI to help you break into the world of Big-Data development.  Backed with Docker, best-in-class open source Big-Data tools and Cisco API’s, Big-Data Scientists and Developers can leverage DLP to easily build solutions, visualize data and turn POC’s for production applications. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/


### APACHE ZEPPELIN - AN INTRODUCTION

Zeppelin is a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. </br>

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelin’s notebooks provides interactive snippet-at-time experience to data scientist.


###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin :

1. Data Exploration and discovery

2. Data Visualization - Tables,graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer : https://zeppelin.apache.org/


## PROCESS OVERVIEW 

In this lab, we will use a Log Simulator, CSA Engine and the customised handler. This lab can be divided in to 3 sections:
</br>

1. Section A: CSA Engine and Log Simulator set-up.

2. Section B: Customised CSA Handler coding and configuration.

3. Section C: Visualisation of output using Zeppelin.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Process6.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the program.

###### Step 1: 

Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology by giving the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

If accessing for the first time, please register and create a new profile:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Register.jpeg?raw=true)


###### Step 2: 

After logging in, user will be directed to the sandbox UI as shown in the screen below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 3:

User needs to click on the Reserve button shown in sandbox page as shown in the screenshot below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 4:

On click of reserve button, a pop up window is shown with the details as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Reserve_page1.jpeg?raw=true)

###### Step 5:

On click of Reserve button on the pop up window, user would receive an email on registered email ID with the login credentials. The email format would be as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/email_2.jpeg?raw=true)

###### Step 6:

User needs to click on the link given in the email. On click of the link, the following page would be shown to the user.

Supply the credentials recieved with the email and click on “Login”.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/dlp login page.jpeg?raw=true)


###### Step 7:

On login, user would be directed to the DevNet DLP (Data Learning Platform) as shown in the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/data-explore-using-zeppelin/assets/Trial1.jpeg?raw=true)

###### Step 8:

Navigate to <b>App Stack Hub</b>, click on Stack library tab and then click on <b>CSA_user</b> stack.
</br>
Two micro-services are listed under this stack. Those are </br>
a) logsimulator </br>
b) trucq </br>

Start both of the micro services as shown in the screenshot below :<br>

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/StartService.PNG?raw=true)

Click on the Green icon shown to start the micro services as shown in the screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/runMicroservices.png?raw=true)

Once the micro services are started, it would be as shown in the screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/CSA_6.png?raw=true)

###### Step 8:

Once the microservices are started and running, we will get IP address and port details. 
For example:</br>

```
     10.10.10.92:11841	    // 11841 is the SQL query Listener Port
```
N.B : The IP and Port details obtained will be required in <b>Step 8 and Step 13</b>

###### Step 9:

Navigate to DLP - Development Hub from the left pane, select CSA customize handler <b>"wksp-csa-handler"</b> and click on <b>"Launch"</b> as shown in the screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/CSAWorkSpaceSelection.PNG?raw=true)

###### Step 10:
On click of launch, user will be navigated to a pre-configured IDE (Integrated Development Environment) as shown in the screenshot below:

![Figure](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/csa-IDE.PNG?raw=true)

<b>Step 5:</b> All the basic configurations are written in “include-handler.xml” file. Navigate to the location - <b>CsaCustomHandler-->build-->tmp-->include-handler.xml.</b> Double click and open it in the right panel.<br>

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/csaHandlerPNG.png?raw=true)

###### Step 11:

In the file “include-handler.xml” set the kafka Topic, Kafka Zookeeper IP and Port and change the name of the custom class which we are going to develop. Here handler java class name is ETLHandler.java.

```
	<topic>CiscoDDP</topic>     // Change the Kafka Topic here.
	<arguments>
 		<arg name="zookeeper.connect">172.16.10.8:2181/kafka</arg>  //Configure IP & Port
	</arguments>
	<custom class="custom.handlers.ETLHandler” </custom>        
	//Set the Handler Java Class which we are going to code next step
```

Sample Java class for CSA handler is placed in below mentioned path. It is named as ETLHandler.<br>
<b>CsaCustomHandler --> customizations --> src --> ETLHandler.java </b>

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/ETLHandlerClass.PNG?raw=true)

Please find the sample code:

```
package custom.handlers;

import org.apache.commons.configuration.SubnodeConfiguration;
import com.cisco.pa.handlers.model.DataRecord;
import com.truviso.system.handlers.AbstractEntryHanlder;
import com.truviso.system.handlers.HandlerException;

public class ETLHandler extends AbstractEntryHanlder<DataRecord, DataRecord> {
	String string;
	public ETLHandler(SubnodeConfiguration c) {
		super(c);
	}
	@Override
	public void init() throws HandlerException {
		super.init();
        try {
        	string = config.getString("string", "POST");
               } catch (IllegalArgumentException e) {}
	}
	@Override
	public void handle(DataRecord record) throws HandlerException {

		try {
			if (null != record) {
				if (record.toString().contains(string))
					resultListener.handle(record);
			}

		      } catch (Exception ex) {throw new HandlerException(ex);}}}
```



###### Step 12:

We need Trucq IP and Port details obtained earlier to set the handler. The configuration file needs to be updated.<br> 

Navigate to the location mentioned below and open the file :

<b>CsaCustomHandler-->customizations-->instances-->local-runtime.properties</b></br>

Please refer step 8 on details to obtain the Trucq port number.

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/aaaaa.png?raw=true)

###### Step 13:

Select "Stop" from the IDE combo box to stop the handler. Please refer the screenshot below:

![Figure](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/step13.jpg?raw=true)

###### Step 14:

Select "Build" from the IDE combo box to build the handler. Please refer the screenshot below:

![Figure](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/develop-stream-handler-cisco-csa/assets/images/step14.jpg?raw=true)

###### Step 15:

Select "Run" from the IDE combo box to start the handler. Please refer the screenshot below:

![Figure](https://raw.githubusercontent.com/prakdutt/data-dev-learning-labs/master/labs/develop-stream-handler-cisco-csa/assets/images/step15.jpg)

Now we have completed all the configurations required as well as the coding. Next, we need to visualize the same using Apache Zeppelin.
</br></br>


