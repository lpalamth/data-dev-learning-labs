# <center>DATA TRANSFORMATION USING TALEND</center>


Data Integration Software Used : Talend

Executed on : Hadoop Big Data Platform


## OVERVIEW

This learning lab can be used as a guide to get a high level understanding to use Talend Open Source Big data tool for performing the transformation operation on CSV data file and save the data to Hadoop platform. We will be using DevNet Data Learning Platform referred as "DLP" during the course.Source data would be selected from DLP's Hadoop platform and the transformation logic would be applied from DLP's browser.

## PRE-REQUISITES

1.	Install Chrome Browser.

2.  Install Java 1.7 version or higher.

2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology (Detailed instructions given below)

3.  Basic knowledge of Apache hadoop and Big Data.

4.  Basic knowledge of ETL (Extract, Transform, Load).  


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with using Talend Open Source Big Data Tool to create a ETL(Extraction, Transformation and Load) Job.

3. To get familiarized with managing (Upload, Execute, Delete, Share) Talend jobs from DLP platform.

4. To get familiarized with transformed data visualization from DLP platform.


## TERMINOLOGIES USED

### DATA TRANSFORMATION - AN INTRODUCTION

Data transformation is the process of converting data or information from one format to another, usually from the format of a source system into the required format of a new destination system. 


### TALEND - AN INTRODUCTION
Talend is a software vendor specializing in Big Data Integration. Talend makes ETL and data management technology easily available to organizations with Talend Open Studio for Data Integration.
The features include:

* A graphical integrated development environment with an intuitive Eclipse-based interface.
* Drag-and-drop job design.
* A unified repository for storing and reusing metadata.
* The broadest data connectivity support of any data integration platform, with more than 900 components and built-in      connectors that let you quickly bridge between databases, mainframes, file systems, web services, packaged enterprise applications, data warehouses, OLAP applications, Software-as-a-Service and Cloud-based applications, and more.
* Advanced ETL functionality including string manipulations, automatic lookup handling, and management of slowly changing dimensions.
* Support for ELT (extract, load, and transform) as well as ETL, even within a single job.

For more details, please refer:
https://www.talend.com/resource/etl.html
https://en.wikipedia.org/wiki/Talend

### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is an integrated data platform from CISCO that includes an easy-to-use UI, Docker-    based infrastructure, best-in-class open-source big-data components, and Cisco’s APIs and tools for data developers and data  scientists who want to develop, validate and provision their solutions before deploying or to explore, analyze, and    visualize their data. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built    on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/

## PROCESS OVERVIEW 

This learning lab would primarily be divided in to 5 different sections:

**Section A:** Download and Install Talend Open Studio.</br>
**Section B:** Develop a Transformation Job using Talend Open Studio from user's local computer. </br>
**Section C:** Build and Export Transformation Job using Talend Big Data Tool to user's local computer. </br>
**Section D:** Upload the transformation job to DLP platform.</br>
**Section E:** Execute transformation from DLP platform and visualise the data. </br>


![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Process7.jpeg?raw=true)

Each of these sections(A, B,C,D and E) are described in details below. Please follow each step carefully.
_____________________________________________________________________________________________________________________________
**SECTION A: DOWNLOAD AND INSTALL TALEND OPEN STUDIO.** 
_____________________________________________________________________________________________________________________________

###### Step 1:

For installing Talend, JDK needs to be pre-installed in your computer. 
To check the Java version existing in your system, please go to command prompt --> Use the command(java -version). This command will show the JDK version installed in user's computer. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/javaVersion.PNG?raw=true)
</br>

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/javaVersionUnix.PNG?raw=true)

If java is not installed, it could be installed from the below mentioned URL: </br>

http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

</br>

###### Step 2:

Next, we need to install Talend Open Source Big data tool. 

1. Please visit the below mentioned website for installing the Talend Open Source Big data tool. 

https://www.talend.com/landing-download-ppc/big-data 

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/DownloadTalendTool.PNG?raw=true)

Input your details required on the form and click on <b>Download Now</b>.The installable will  be downloaded a zip file. 
</br></br>

2. Unzip the downloaded zip file and run the executable file.Please refer the screenshot below :</br>

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/CreateTalendJob.PNG?raw=true)
</br>

3. User can create a New Talend Project by selecting <b>Create a New Project.</b> and giving a Project name in the specified text box as shown in screenhot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/CreateTalendJob.PNG?raw=true)


4. A new Talend project will be created and the designer UI will appear. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/talendUIdesign.png?raw=true)
</br>

5. A new transformation job needs to be created from the job creation panel in Talend. For designing a new job, the user can drag and drop different components from designer palette. </br>

Talend Open source Big data tool provides 900+ components for different transformation operation. The User can select component(s) from this palette and place those in the canvas area and connect them. Execution of jobs should be done based on the connection of each component which describes the data flow as well.
</br>
</br>

_____________________________________________________________________________________________________________________________
**SECTION B : DEVELOP A TRANSFORMATION JOB USING TALEND OPEN STUDIO FROM USER'S LOCAL COMPUTER.** 
_____________________________________________________________________________________________________________________________

In this section, we will learn the process of creating a new transformation Job using Talend Open Studio for Big data.We will perform a transformation on CSV file by selecting few specific columns from the source file and save the transformed output file in DLP's hadoop platform after editing few fields.** </br></br>

###### Step 1:

Create a Talend job. Right click on <b>Job Designs</b> from left panel and click on <b>Create Job</b> as shown in screenshot below :

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/createJobStep1.PNG?raw=true)
</br></br>

###### Step 2:

Provide a Job name and other details as required.Click on the <b>Finish</b> button after entering all the necessary details in that form.

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/TalendJobNameAndDetails.PNG?raw=true)

Now a new job is created and it can be viewed under <b>Job Designs</b> section in the red marked area in Talend tool.Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/TalendjobDisplayingJobDesigns.png?raw=true)
</br></br>

###### Step 3:

Navigate to DLP platform. Please follow the steps given below:

* Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology </br>. 

Give the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

If accessing for the first time, please register and create a new profile:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Register.jpeg?raw=true)


* After logging in, user will be directed to the sandbox UI as shown in the screen below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

* User needs to click on the Reserve button shown in sandbox page as shown in the screenshot below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

* On click of reserve button, a pop up window is shown with the details as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Reserve_page1.jpeg?raw=true)

* On click of Reserve button on the pop up window, user would receive an email on registered email ID with the login credentials. The email format would be as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/email_2.jpeg?raw=true)

* User needs to click on the link given in the email. On click of the link, the following page would be shown to the user.

Supply the credentials recieved with the email and click on “Login”.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/dlp login page.jpeg?raw=true)


* On login, user would be directed to the DevNet DLP (Data Learning Platform) as shown in the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/DLP Main screen.jpeg?raw=true)

###### Step 4:

Select the file "Users11.csv" from DLP platform as source file. This CSV file has multiple columns and we need to pickup few columns and generate a new file with the data. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/Users11csvfile.png?raw=true)

###### Step 5:

Preview the file as shown in screenshot below
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/Users11csvfileRecords.PNG?raw=true)
</br>

###### Step 6:

Next, we need to design a job in Talend tool.Drag and drop components in designer canvas from pallate. </br>
The below mentioned components need to be added in the designer area:

* tHDFSConnection : This component is use to make a connection with Hadoop platform.</br>
* tHDFSInput :This component is use to read a file from Hadoop platform. </br>
* tHDFSOutput :This component is use to write a file in Hadoop platform.</br>
* tJavaRow :This component is use to write custom java code.</br>
* tMap :This component is use to map fields from two differnet data source.</br>

After placing the above mentioned components in the designer area, the UI would looks as shown below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/TalendJobcomponents.PNG?raw=true)

###### Step 7:

Click on <b>Contexts(TalendNewJob)</b> tab from the Talend tool. Here <b>TalendNewJob</b> is the talend job name. Contexts tab is an area where we can define the parameters that we want to pass to talend job. For our talend job, we need to pass four(4) parameters. Click on the <b>+</b> symbol four times and it will create placeholders for four parameters. Click on each parameter name and change the name as described in the below image. 

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/contextvariables.png?raw=true)

The context parameters are as shown below :

* hdfsuri_namenodepath: This parameter is used to contain the hdfs name node URI. Syntax of this parameter is hdfsuri_[any text].</br>

* hdfspass_namenodepass: This parameter is used to contain the name node passwords. Syntax of this parameter is hdfspass_[any text]. </br>

* input_hdfsinputfile: This parameter is used to contain the hdfs path of user selected file. In this example, Users11 csv file path would be saved in this parameter. Syntax of this parameter is input_[any text].</br>

* output_hdfsoutputfile: This parameter is used to contain hdfs path for output file name which should be generated by transformation job. Syntax of this parameter is output_[any text].</br>

###### Step 8:

<b>Double click</b> on <b>tHDFSConnection</b> component. Component section will appear in Talend designer studio. We need to set properties for the selected component. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/ComponentProperty.PNG?raw=true)
</br>
The properties to be set are as follows :

* Change the Distribution to Cloudera as our DLP platform is build on Cloudera. </br>

* Change Distribution version to Cloudera CDH5.4. </br>

* Change name node uri as <b>context.hdfsuri_namenodepath</b> </br>

* Change User name as <b>context.hdfspass_namenodeusername</b> </br>

* <b>Use Datanode Hostname</b> should be unchecked.

</br>

###### Step 9:

After setting the tHDFSConnection component properties, we have to set the required properties for tHDFSInput component.
<b>Double click</b> on tHDFSInput and it will display the Component tab as shown in the screenshot below. 

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/ComponentPropertytHDFSInput.PNG?raw=true)
</br>
The below mentioned changes needs to be applied to tHDFSInput component tab. </br>

* select the checkbox - <b>Use an existing Connection.</b>.  </br>

* Change the <b>File name</b> as context.input_hdfsinputfile. </br>

* <b>Field seperator</b> should be comma(,).

</br>
###### Step 10:

Users11.csv is the source file available in DLP platform and has multiple columns/fields. We need to set the columns/fields required in tHDFSInput component.

* Double click on tHDFSInput component and it will open the Component's property section.
* Click on <b>ellipsis</b> button in <b>Edit Schema.</b> 
* An entry form will open where user can define the schema that is pre-defined in the source file(Users11.csv)
* Click on <b>+</b> symbol and user can enter the field names accordingly. 

Please refer the field names that is entered as an example:</br>

```json
UserID,
Email,
Username,
Country,
AccessLevel,
LastLoginDate,
FirstName,
LastName
```
N.B: There should not be any space in Field name.

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/InputFileSchema.PNG?raw=true)

Once the field names are added, click on <b>OK</b> to save the schema for tHDFSInput component.

###### Step 11:

After setting the tHDFSInput component properties, now we have to set the required properties for tHDFSOutput component.<b>Double click</b> on the component which will display the Component tab.

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/ComponentPropertytHDFSoutput.PNG?raw=true)

The below mentioned changes needs to be applied to tHDFSoutput component tab. </br>

* Select the checkbox - <b>Use an existing Connection.</b> </br>

* Change the <b>File name</b> as context.output_hdfsoutputfile </br>

* <b>Field seperator</b> should be comma(,). </br>

###### Step 12:

Next, we need to set schema in tHDFSOutput component based on Input file.</br> Similarly, user need to set the schema for output file in tHDFSOutput component. 

* Double click on tHDFSOutput component and it will open the Component property section. 
* Click on <b>ellipsis</b> button in <b>Edit Schema</b> section that will allow user to enter the field names for output file.
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/outputSchema.PNG?raw=true)
</br></br>

Please refer the field names that is entered as an example:</br>


```json
UserID,
Email,
Username,
Country,
AccessLevel,
LastLoginDate,
FirstName,
LastName,
FullName
```

In the example shown above, we are trying to merge First Name and Last Name into one field called FullName in the output file.</br>
</br>
</br>

###### Step 13:

Next, we need to make connection between each component based on the data flow.


* Right Click on <b>tHDFSConnection</b> component and select <b>Trigger->On Subjob OK</b>.

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/thdfsConnection1.PNG?raw=true)

* Click on <b>tHDFSInput</b>. This will make an arrow from <b>tHDFSConnection</b> to <b>tHDFSInput</b>. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/connector03.PNG?raw=true)

* Right click on <b>tHDFSInput</b> component and select <b>Row->Main</b> and then click on <b>tHDFSOutput</b> compoment. Please refer the screenshot below:

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/connector04input.png?raw=truee)

Follow the steps above to build connection from one component to another component. Flow of data would be as mentioned below</br>

* </b>tHDFSConnection_1->tHDFSInput_1->tJavaRow_1->tMap_1->tHDFSOutput_1</b> </br>

* <b>Output connector name from tMap_1 to tHDFSOutput is output. When connecting tMap to tHDFSOut, prompt message will be shown to the user asking for the output connector name. Enter "output" as a output connector name.</b>

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/connectionFinalImage.PNG?raw=true)

###### Step 14:

Click on <b>tJavaRow</b> components. Click on <b>Edit Schema</b> and add a new field - <b>FullName</b> and set the <b>Type</b> as <b>String</b>. Add the below mentioned code in the code area. Please refer the screenshot below :</br>

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/tJavaRowJavaCode.PNG?raw=true)

```json
//Code generated according to input schema and output schema
output_row.UserID = input_row.UserID;
output_row.Email = input_row.Email;
output_row.Username = input_row.Username;
output_row.Country = input_row.Country;
output_row.AccessLevel = input_row.AccessLevel;
/*output_row.LastLoginDate = TalendDate.parseDate("MM/dd/yyyy",input_row.LastLoginDate).toString();*/
output_row.LastLoginDate = input_row.LastLoginDate;
output_row.FirstName = input_row.FirstName;
output_row.LastName = input_row.LastName;
output_row.FullName = StringHandling.UPCASE(input_row.FirstName + " " + input_row.LastName);
```

###### Step 15:

Click on <b>tMap_1</b> component and map the fields by draging from left to right. Click on <b>Country</b> field and add the  line of code shown below (Please refer screenshot below for more details). 

![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/data-transformation-with-talend/assets/images/tmapConnector.PNG?raw=true)

This will print <b>NoCountry</b> as text where in source file, Country field is empty. 
</br>

###### Step 16:

Click <b>Edit Schema</b> on <b>tHDFSOutput_1</b> and Sync the Schema.

</br>

## LESSONS LEARNT 

To create a Talend job in Talend Open source big data tool. 

NEXT ?

Learn the process to build a job and export that job in zip format inorder to upload the same to DLP platform for further processing.
