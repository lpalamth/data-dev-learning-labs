# <center>NETWORK DATA INGESTION AND TRANSFORMATION</center>

<b>Executed on</b>: Hadoop Big Data Platform

<b>Visualization tool used</b>: Tableau


## OVERVIEW

This learning lab can be used as a guide to get a high level understanding on the process of ingesting network data in to Hadoop environment, perform transformation & extraction and visualize using an analytical tool called Tableau. We will be using DevNet Data Learning Platform referred as "DLP" during the course. In this lab, the network stream data is already pre-created. From DLP platform, the user can access it directly. 

Please refer the example shown below to get a high level understanding :

![alt tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/network-data-transformation/assets/images/flow1.png?raw=true)

A data collection server, as shown in the diagram below, is collecting data in real time from the local network. The data collected by the Server is working with a Client residing in DLP to transfer the network data collected through Kafka. Using Kafka socket code, we are making a connection to the client that captures the network data and sends it to a Kafka topic. From this topic, data will be moved to HDFS by a Consumer program. With the data present in HDFS, we can further transform it and visalize in different formats such as pie chart, bar chart etc. using Tableau tool. The diagram shows how exactly network data flows from a local network through Kafka in HDFS and gets transformed.


## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology (Detailed instructions given below)

3. Basic Knowledge of data storage on Hadoop.

4. Basic knowledge of Apache Spark.


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with the ways to get network data from HDFS. 

3. To get familiarized with the process to transform network data by creating transform function using IDE.

3. To get familiarized with data visualization using Tableau.


## TERMINOLOGIES USED


### WHAT IS NETWORK DATA? - AN INTRODUCTION

The Computer network is a telecommunication process which allows computers or devices to exchange data between each other using data pipeline and those devices that are controlled by wired or wireless medium. Those devices are kept alive by exchanging data between each other in a continuous way. 

These network data provide the inside details about communication performance between of two devices that are communicating. We can extract lots of valuable information from those data set if we can capture those data in real time. 

## APACHE KAFKA? - AN INTRODUCTION

Kafka is a distributed streaming platform that is designed to be fast, scalable, and durable. It has 3 key capabilities:

1.	It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.
2.	It lets you store streams of records in a fault-tolerant way.
3.	It lets you process streams of records as they occur.

It gets used for two broad classes of application:

1.	Building real-time streaming data pipelines that reliably get data between systems or applications
2.	Building real-time streaming applications that transform or react to the streams of data

For more details, please refer:
https://kafka.apache.org/

### TABLEAU - AN INTRODUCTION

Tableau is a powerful business intelligence and analytics tool. There are mainly five main products catering to diverse visualization needs for professionals and organizations. They are:

Tableau Desktop: Made for individual use
Tableau Server: Collaboration for any organization
Tableau Online: Business Intelligence in the Cloud
Tableau Reader: Let you read files saved in Tableau Desktop.
Tableau Public: For journalists or anyone to publish interactive data online.

Tableau Desktop is a data visualization application that lets you analyze virtually any type of structured data and produce highly interactive, beautiful graphs, dashboards, and reports. You can connect to virtually any data source from spreadsheets to data warehouses and display information in multiple graphic perspectives. 

For more details, please refer : 
http://www.tableau.com/learn/training
https://community.tableau.com/welcome


### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is an integrated data platform from CISCO that includes an easy-to-use UI, Docker-    based infrastructure, best-in-class open-source big-data components, and Cisco’s APIs and tools for data developers and data  scientists who want to develop, validate and provision their solutions before deploying or to explore, analyze, and    visualize their data. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built    on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/

## PROCESS OVERVIEW 

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Process9.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

###### Step 1: 

Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology by giving the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

If accessing for the first time, please register and create a new profile:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Register.jpeg?raw=true)


###### Step 2: 

After logging in, user will be directed to the sandbox UI as shown in the screen below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 3:

User needs to click on the Reserve button shown in sandbox page as shown in the screenshot below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 4:

On click of reserve button, a pop up window is shown with the details as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Reserve_page1.jpeg?raw=true)

###### Step 5:

On click of Reserve button on the pop up window, user would receive an email on registered email ID with the login credentials. The email format would be as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/email_2.jpeg?raw=true)

###### Step 6:

User needs to click on the link given in the email. On click of the link, the following page would be shown to the user.

Supply the credentials recieved with the email and click on “Login”.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/dlp login page.jpeg?raw=true)


###### Step 7:

On login, user would be directed to the DevNet DLP (Data Learning Platform) as shown in the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/data-explore-using-zeppelin/assets/Trial1.jpeg?raw=true)

The first tab on left pane - Data Repository is shown to the user. Data Repository section allows to create network real-time data stream. Kafka's producer will push the Network traffic generated data to Kafka cluster and Consumer can consume that data and save that in to HDFS real-time. From HDFS, we can visualise the data using visualization tool like Tableau. This application will allow you to use the existing Kafka's Producer & Consumer function to ingest your real-time network data into HDFS environment.

<b>DLP platform provides a default Kafka's Producer, Consumer and Network traffic generator</b>.The user can select their own network data and use DLP provided Kafka's component to process the network data. User can define and deploy their own service as well. 


###### Step 8:

Select the tab Development Hub on the left pane as shown in screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Development Hub.jpeg?raw=true)

###### Step 9:


Select the work space - <b>"wksp-transform"</b> and click on Launch button as shown in screenshot below: 

![alk-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/network-data-transformation/assets/images/WorkSpaceSelection.PNG?raw=true)
