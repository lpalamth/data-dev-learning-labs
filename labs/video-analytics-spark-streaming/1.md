

# **Video Analytics Using Spark Streaming**

# **Intorduction about Spark Streaming and Real time Data processing.**
Spark Streaming is a key component of Spark, a hugely successful open project in the recent years. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map-reduce, join and window. Finally, processed data can be pushed out to file systems, databases, and live dashboards.

# **Lab Overview**
Our application is using micro service for video data ingestion. It will read the video data from mockup video source and forward the data to Kafka broker. Our sample code will read the video data from Kafka broker and detect the face image and save that image(s) into HDFS platform. User can view the image(s) from DLP platform.

In this lab we will integrate Spark Streaming and Kafka, detect a human face using OpenCV from a video streaming, and then save the results to a file in Hadoop environment.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/video-image-capturing.png?raw=true)

## **Lab Objectives**

- Learn how to integrate the SparkStreaming with Kafka
- Learn how to perform video analytic functions in a real-time stream application
- Learn how to save a file to HDFS

## **Prerequisites**

- Knowledge of Java language, Maven
- Knowledge for OpenCV face detection function
- Studied platform user guide
- Basic knowledge of how Spark Streaming works
- Chrome Browser


## Step 1: Explore Data Learning Platform(DLP)

<font color='red'>Request access to the Data Learning Platform by sending a message to:</font> [datalearningplatform@cisco.com](mailto:datalearningplatform@cisco.com)

1. In DLP website, go to the development area by clicking <b>"Development Hub"</b>. 
2. Select the workspace called <b>"wksp-video-lab"</b> and Click on <b>"Launch"</b> button.
3. It will open a new browser tab with pre-loaded video streaming code.

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/WorkspaceSelection.PNG?raw=true)

N.B: This sample code can be used for exploring the process.

## Step 2: Sample Code for Detecting face from mockup video source.

<b>Our video streaming program is used to detect the video streaming from a source. Below Java program is used to read the video data, then detect the face and save the face images to HDFS.</b><br>

Select App.java from Left Panel and the code will open on right Panel.
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/sourceCode.PNG?raw=true)


```json
package DDP.VideoLabs;

import java.io.BufferedInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URI;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import org.joda.time.DateTime;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import org.opencv.core.Core;
import org.opencv.core.Mat;
import org.opencv.core.MatOfRect;
import org.opencv.core.Size;
import org.opencv.imgcodecs.Imgcodecs;
import org.opencv.imgproc.Imgproc;
import org.opencv.objdetect.CascadeClassifier;

import kafka.serializer.DefaultDecoder;
import kafka.serializer.StringDecoder;
import scala.Tuple2;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

public class App 
{
    public static void main( String[] args ) throws InterruptedException
    {
    	if (args.length != 3)
    	{
    		System.out.println("please set parameter value for kafka and topic and username");
    		return;
    	}
    	String kafka = args[0];
        String topics = args[1];
        final String username = args[2];
        
      	// Create context with a 2 seconds batch interval
        SparkConf sparkConf = new SparkConf().setAppName("JavaVideoLabs");
        final JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));

        HashSet<String> topicsSet = new HashSet<String>(Arrays.asList(topics.split(",")));
        HashMap<String, String> kafkaParams = new HashMap<String, String>();
        kafkaParams.put("metadata.broker.list", kafka);
        kafkaParams.put("group.id", "groupid");
        kafkaParams.put("consumer.id", "consumerid");

        // Create direct kafka stream with brokers and topics
        JavaPairInputDStream<String, byte[]> messages = KafkaUtils.createDirectStream(
                jssc,
                String.class,
                byte[].class,
                StringDecoder.class,
                DefaultDecoder.class,
                kafkaParams,
                topicsSet
        );
        

                  
        JavaDStream<String> content = messages.map(new Function<Tuple2<String, byte[]>, String>() {
           	private static final long serialVersionUID = 1L;
			//@Override
            public String call(Tuple2<String, byte[]> tuple2) throws IOException {
                System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
                if ((tuple2 == null) || (tuple2._2().length < 1000))
                	return "";
                
                Mat image = new Mat(new Size(640, 480), 16);
                image.put(0, 0, tuple2._2()); 
                                    
                // Detect faces in the image.
                Mat mGrey = new Mat();
                Imgproc.cvtColor( image, mGrey, Imgproc.COLOR_BGR2GRAY); 
                CascadeClassifier faceDetector =
                        new CascadeClassifier(GetResourceFilePath("/haarcascade_frontalface_alt.xml").toString());
                MatOfRect faceDetections = new MatOfRect();
                faceDetector.detectMultiScale(mGrey, faceDetections);
                int len = faceDetections.toArray().length;
                System.out.println(String.format("Detected %s faces", len));
                if (len > 0)
                {
	                SaveImageToHDFS(image, username);
	                return "face";
	              
                } else
                    return "";
           
            }
        });
        
        DateTime start = new DateTime();
        
        content.count().print();
       
        jssc.start();
        DateTime end;
        while (true) {
            end = new DateTime();
            if (end.getMillis()-start.getMillis() > 60000) {
                jssc.stop(true, true);
                break;
            }

        }

        jssc.awaitTermination();
    }
    
    public static void SaveImageToHDFS(Mat img, String username) throws IOException
    {
    	 DateTime now = new DateTime();
    	 int year = now.getYear();
    	 int month = now.getMonthOfYear();
    	 int day = now.getDayOfMonth();
    	 int hour = now.getHourOfDay();
    	 int minute = now.getMinuteOfHour();
    	 int second = now.getSecondOfMinute();
    	 String time = ""+year+month+day+hour+minute+second;
    	 String tmpFile = "/tmp/" + username + time + ".jpg";
         Imgcodecs.imwrite(tmpFile, img);
         String hdfsAddr = "hdfs://172.16.1.11:8020/ddp/"+username+"/"+time+".jpg";
         Configuration config = new Configuration();
         FileSystem fs = FileSystem.get(URI.create(hdfsAddr), config);
         Path path = new Path(hdfsAddr);
         FSDataOutputStream out = fs.create(path);
         InputStream is = new BufferedInputStream(new FileInputStream(tmpFile));
         IOUtils.copyBytes(is, out, config);
         is.close();
         out.close();
         fs.close();
         
         File f = new File(tmpFile);
     	 if (f.exists() && !f.isDirectory())
     		f.delete();
         System.out.println("write file to hdfs");
    }
    
    public static String GetResourceFilePath(String filename) {
        InputStream inputStream = null;
        OutputStream outputStream = null;
        String tempFilename = "/tmp" + filename;
        try {
        	File f = new File(tempFilename);
        	if (f.exists() && !f.isDirectory())
        		return tempFilename;
        	
            // read this file into InputStream
            inputStream = App.class.getResourceAsStream(filename);
            if (inputStream == null)
                System.out.println("empty streaming");
            // write the inputStream to a FileOutputStream
            outputStream =
                    new FileOutputStream(tempFilename);

            int read;
            byte[] bytes = new byte[102400];

            while ((read = inputStream.read(bytes)) != -1) {
                outputStream.write(bytes, 0, read);
            }
            outputStream.flush();

            System.out.println("Load XML file, Done!");

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            if (inputStream != null) {
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (outputStream != null) {
                try {
                    // outputStream.flush();
                    outputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }

            }

        }
        return tempFilename;
    }
    
}


```   

## Step 3: Maven package and submit the spark task to Hadoop cluster.

Once we finish the above code, we can select <b>“package”</b> in the CMD menu, and click on <b>Blue Button</b> ![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/BlueButton.PNG?raw=true). This will package the application.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/BuildProject.PNG?raw=true)

After successful packaging, select <b>run.sh</b> from left panel and select <b>“run”</b> and click the blue button again to execute it.

![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/scriptToRun.PNG?raw=true)

<b>Input parameters are used in run.sh</b>.  <br>
172.16.11.7:9092 -> kafka broker<br>
ddp_video -> kafka topic <br>
user_2 -> login username. <br>
This script (run.sh) submit the spark task to cluster.


## Step 4: Start the Video source micro service from DLP platform.
Below steps is using to start the micro service to push the video streaming data to Kafka broker. </br>
1. From DLP platform, clicking on  <b>App Stack Hub</b>, it will open an UI in right panel. Select <b>Stack Library</b> and select <b>videolabs_User_7</b>. </br>
   N.B: Here <b>videolabs_User_7</b> is names with Logged UserID. If User_2 logged in, then this text would be as <b>videolabs_User_2</b></br>
2. Check the <b>Health</b>  status. It shoul be in <b>Stop</b> status at the beginning.</br> If the status is in <b>Running</b> state, it is required to stop first and then start the service. </br>
3. Click on Right green button to start the service.  </br>
   N.B: For learning, we only use small set of data for streaming.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/VideoVideoSrouce.PNG?raw=true) </br>
4. On clicking on Green action button, the status of <b>Health</b>  would be changed to <b>waiting</b> state first. </br>
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/VideoVideoSrouceWaitingState.PNG?raw=true) </br>
5. After the waiting state, status of <b>Health</b> would be in <b>Running</b> state. </br>
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/VideoVideoSrouceRunningState.PNG) </br>

## Step 5: View the file which has face images in DLP Platform

The spark task detect the face(s) from the video streaming and save them into hadoop file system, you can view them in DLP following the below steps. 

1. From the DLP home page, go to the <b>Data Repository</b>.
2. Find the image file in the data list.
3. Select the image file, then click on preview button.  
![alt-tag](https://github.com/prakdutt/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/previrewImage.PNG?raw=true)
4. After clicking on the preview button, image would be display in popup.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/face.jpg?raw=true)

## Step 6: Stop the video source micro service.
After displaying the images from DLP environment, we need to stop the video source micro-service from <b>App Stack Hub</b>. Click on Red Stop button once.
![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/video-analytics-spark-streaming/assets/VideoVideoSrouceStopState.PNG)

**Input** : The stream data from mockup video souce from DLP.  
**Output** : The image(s) file(s) which has face in HDFS, and you can preview it in DLP.  

From this we have learned how to how to integrate the SparkStreaming with Kafka and how to save the file to HDFS.

There are some more examples and exercise are available in the below mentioned link. This is for your reference.

[http://spark.apache.org/docs/latest/streaming-programming-guide.html](http://spark.apache.org/docs/latest/streaming-programming-guide.html)

