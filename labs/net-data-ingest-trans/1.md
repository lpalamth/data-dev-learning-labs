# <center>NETWORK DATA INGESTION</center>

<b>Stream Processing Platform Used</b>: Apache Kafka

<b>Executed on</b>: Hadoop Big Data Platform


## OVERVIEW

This learning lab can be used as a guide to get a high level understanding on the process of ingesting network data in to Hadoop environment. We will be using DevNet Data Learning Platform referred as "DLP" during the course. In this lab, the network stream data is already pre-created. From DLP platform, the user can access it directly. A data collection server, as shown in the diagram below, is collecting data in real time from the local network. The data collected by the Server is working with a Client residing in DLP to transfer the network data collected through Kafka. Using Kafka socket code, we are making a connection to the client that captures the network data and sends it to a Kafka topic. From this topic, data will be moved to HDFS by a Consumer program. The diagram shows how exactly network data flows from a local network through Kafka in HDFS and gets transformed.

Please refer the example shown below to get a high level understanding :

![alt tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/network-data-transformation/assets/images/flow1.png?raw=true)


## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology (Detailed instructions given below)

3. Basic Knowledge of data storage on Hadoop.

4. Basic knowledge of Apache Kafka.


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with the ways to get network data from HDFS. 

3. To get familiarized with the ways to ingest network streaming data.

4. To get familiarized with the process of configuring Kafka and network traffic simulator.

5. To get familiarized with the method of visualizing network data from DLP's platform.


## TERMINOLOGIES USED


### WHAT IS NETWORK DATA? - AN INTRODUCTION

The Computer network is a telecommunication process which allows computers or devices to exchange data between each other using data pipeline and those devices that are controlled by wired or wireless medium. Those devices are kept alive by exchanging data between each other in a continuous way. 

These network data provide the inside details about communication performance between of two devices that are communicating. We can extract lots of valuable information from those data set if we can capture those data in real time. 

## APACHE KAFKA? - AN INTRODUCTION

Kafka is a distributed streaming platform that is designed to be fast, scalable, and durable. It has 3 key capabilities:

1.	It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.
2.	It lets you store streams of records in a fault-tolerant way.
3.	It lets you process streams of records as they occur.

It gets used for two broad classes of application:

1.	Building real-time streaming data pipelines that reliably get data between systems or applications
2.	Building real-time streaming applications that transform or react to the streams of data

For more details, please refer:
https://kafka.apache.org/


### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is an integrated data platform from CISCO that includes an easy-to-use UI, Docker-    based infrastructure, best-in-class open-source big-data components, and Cisco’s APIs and tools for data developers and data  scientists who want to develop, validate and provision their solutions before deploying or to explore, analyze, and    visualize their data. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built    on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/

## PROCESS OVERVIEW 

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Process8.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

###### Step 1: 

Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology by giving the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

If accessing for the first time, please register and create a new profile:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Register.jpeg?raw=true)


###### Step 2: 

After logging in, user will be directed to the sandbox UI as shown in the screen below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 3:

User needs to click on the Reserve button shown in sandbox page as shown in the screenshot below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 4:

On click of reserve button, a pop up window is shown with the details as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Reserve_page1.jpeg?raw=true)

###### Step 5:

On click of Reserve button on the pop up window, user would receive an email on registered email ID with the login credentials. The email format would be as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/email_2.jpeg?raw=true)

###### Step 6:

User needs to click on the link given in the email. On click of the link, the following page would be shown to the user.

Supply the credentials recieved with the email and click on “Login”.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/dlp login page.jpeg?raw=true)


###### Step 7:

On login, user would be directed to the DevNet DLP (Data Learning Platform) as shown in the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/data-explore-using-zeppelin/assets/Trial1.jpeg?raw=true)

The first tab on left pane - Data Repository is shown to the user. Data Repository section allows to create network real-time data stream. Kafka's producer will push the Network traffic generated data to Kafka cluster and Consumer can consume that data and save that in to HDFS real-time. From HDFS, we can visualise the data using visualization tool like Tableau. This application will allow you to use the existing Kafka's Producer & Consumer function to ingest your real-time network data into HDFS environment.

<b>DLP platform provides a default Kafka's Producer, Consumer and Network traffic generator</b>.The user can select their own network data and use DLP provided Kafka's component to process the network data. User can define and deploy their own service as well. 
</br>


###### Step 8:

For live stream data ingestion into DLP platform, we need to configure KAFKA. Please follow the steps mentioned below:

* In Data Repository tab, click on <b>Import New Data</b> button from <b>Data Repository</b> tab.

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData.PNG?raw=true)

* It will open a pop-up form where user needs to set the properties - KAFKA and NTSERVER. 

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData2.PNG?raw=true)

<b>name</b>-> Any text as a name </br>

<b>Type </b>-> It is a drop-down list box. Select <b>network</b> from the list. </br>
<b>Select Streaming Data Processing Service</b> -> Select default value from the list. </br>
<b>TOPIC</b> -> Change the topic name as <b>DDP_user_2</b>. If your login id is <b>user_2</b>. If your login id if <b>user_3</b>, then the value of this field would be <b>DDP_user_3</b>. </br>
<b>Description </b> -> Put some details about your live stream. </br>

**Keep the default value of KAFKA and NTSERVER. Don't change the text against those two fields.**

###### Step 9:


Next, we need to create stack for configuring consumer in DLP Platform. Consumer will consume network data and help to preserve into DLP. Please follow the steps mentioned below:

* Navigate to App Stack Hub tab in left pane. Click on <b>Create Stack</b> button. </br>

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData4.PNG?raw=true)

* Create Stack Entry pop-up will open. Enter the details as shown below: </br>

<b>Stack name</b>-> Enter any text as a Stack name.</br>

<b>Rack Host</b>-> Select <b>sh_rack_1</b> </br>

<b>Running Platform</b>->Select <b>Mesos</b> from the radio button. </br>

<b>Stack Icon</b>-> Select <b>Common</b> from the list box. </br>

<b>Description</b>->Enter description data as a description of this Stack. </br>

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData5.PNG?raw=true)

* Drag and drop <b>network-stream</b> and <b>ntconsumer</b> into the Canvas from <b>DataSource List</b> and <b>Service List</b> section. </br>

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData6.PNG?raw=true)

* Click on <b>network-stream</b> and then click on <b>Config</b> button. <b>Config</b> button is on right side of the screen. This will open an application Configuration window where we need to set below values. </br>

1. <b>Memory(MB)</b> -> Set <b>256</b> as a memory space. </br>
2. <b>TOPIC</b> -> Change the Topic name as <b>DDP_user_2</b>. Here <b>user_2</b> is the user id which is used for login. If user login as <b>user_3</b> then this value would be <b>DDP_user_3</b> </br>
3. Click on <b>Submit</b> button.  </br>
*No edit is required for other fields for this learning lab.*

![network-data](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData13.PNG?raw=true)

* Click on <b>ntconsumer</b> and click on <b>Config</b> button. This will open an application configuration window. User need to set the below mentioned values there. </br>

1. <b>Memory(MB)</b>-> Set <b>256</b> as a memory space. </br>
2. <b>hdfs_address</b> -> Set the Value as hdfs://172.16.10.16:8020/ddp/[user id]/[any text]. If user's login id is <b>user_2</b> and the output file name is <b>livedata</b> then the value of <b>hdfs_address</b> would be hdfs://172.16.10.16:8020/ddp/user_2/livedata.</br>
3. <b>topic_name</b> -> Set the value as <b>DDP_user_2</b>. </br> Here <b>user_2</b> is the user id which is used for login. If user login as <b>user_3</b> then this value would be <b>DDP_user_3</b> </br>
4. <b>user_name</b> -> Set the login id in this field. Here user login as <b>user_2</b>. </br>
5. Click on <b>Submit</b> button. </br>

###### Step 10:

Start two services from <b>App Stack Hub-> Stack Library</b>. Please follow the steps given below:

a. From <b>Stack Library</b>, click on the Stack name which was created in <b>Step2-4</b>. </br>
It will show the two services where <b>Health</b> must be in <b>Stop</b> state. Find the below screen for the reference.
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData8.PNG?raw=true)

b. Click the <b>Green Start</b> button to start both services. The status of <b>Health</b> would be in <b>Running</b> stage.</br> Find below image for reference. 
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData14.png?raw=true)
*This will start ingesting live stream data into DLP platform. User can view the live stream data from* <b>Data Repository</b>.
</br>
c. Click <b>Data Repository</b> and select the file to view the network stream data. 
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData10.PNG?raw=true).
Click on view button with an eye symbol to display the network data.
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData15.PNG?raw=true)
</br>


###### Step 10:

After viewing the network data from <b>Data Repository</b>, stop the services from <b>App Stack Hub -> Stack Library</b> by clicking on <b>Stop</b> button. 
![alt-img](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/net-data-ingest-trans/assets/images/importNetworkData12.PNG?raw=true)

## LESSONS LEARNT:

1. How to get network data from HDFS. 

2. How to ingest network streaming data.

3. How to configure Kafka and network traffic simulator.

4. How to visualize network data from DLP's platform.


# <center>Congratulations! You have successfully completed the Learning Lab!

