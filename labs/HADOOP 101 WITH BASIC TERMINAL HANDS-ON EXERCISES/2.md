
### HOW TO USE APACHE SPARK-SHELL TO INTERACT WITH SPARK CLUSTER###

###<b> Step 1 : Echo command to find HDFS path</b>

From Hadoop Terminal, use echo command to find the path as shown below:

```jason
echo $HDFS
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic29.png?raw=true)

Note: The path obtained needs to be used in commands shown below to replace {$HDFS}.


### <b>Step 2 : Launching Spark shell interactive environemnt</b>

Spark shell interactive environment needs to be launched from terminal.Please follow the steps given below. User could launch the Spark shell interactive environment either in local mode or in cluster mode but not in both of them.

To launch in Local Mode, please follow the command given below:

```jason
spark-shell
```

To launch in cluster Mode, please follow the command given below:

```jason
spark-shell --master yarn --deploy-mode client
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic53.png?raw=true)

The spark shell prompt is as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic54.png?raw=true)

### <b>Step 3: Word Count transformation program with basic spark code</b>

Execute word count transformation program with basic spark code as follows. The commands mentioned below needs to be executed in sequential order.

* Load input data from a text file:

```jason
val inputfile = sc.textFile("${HDFS}hadoop-learning-labs-people.txt") 
```
Replace the hdfs path with the path obtained with Echo command in Step 1.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic25.png?raw=true)

* Read an input set of text and split the data in to words:

```jason
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic26.png?raw=true)

* Counts the number of times each word appears and transform it in to word and count.

```jason
counts.cache()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic27.png?raw=true)

* Print the output to terminal.

```jason
counts.toArray().foreach(println)
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic28.png?raw=true)


<b>Steps Performed Summary:</b>

1. Load input data from a file.
2. Read an input set of text and split the data in to words.
3. Counts the number of times each word appears and transform it in to word and count.
4. Print the output to terminal.


### <b>Step 4 : Using SQL to do analytics in Spark shell</b>

There are two different ways:

1. Using Spark SQL
2. Using Normal SQL

The commands mentioned below needs to be executed in sequential order. 

<b>Using Spark SQL</b>

* Create an SQL context:

```jason
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic30.png?raw=true)

* Create a dataframe using the SQL context and data from the file existing in HDFS – “hadoop-learning-labs-people.json”:
  (DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a    relational database)

```jason
val df = sqlContext.jsonFile("${HDFS}hadoop-learning-labs-people.json ")
```
Replace the hdfs path with the path obtained with Echo command in Step 1 OR follow the process below. In order to execute the above mentioned command, user needs to be in spark shell.

```jason
exit
```
```jason
echo $HDFS
```
After obtaining the path,
```jason
spark-shell
```
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic31.png?raw=true)

* Execute command to display the contents of dataframe:

```jason
df.show()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic32.png?raw=true)

* Execute command to select and display the name:

```jason
df.select("name").show()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic33.png?raw=true)

* Execute command to select and display the name and age:

```jason
df.select(df("name"), df("age") + 1).show()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic34.png?raw=true)

* Execute command to select and display the name and age with the filter applied as age greater than 21:

```jason
df.filter(df("age") > 21).show()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic35.png?raw=true)

* Execute command to select and display the name and age with the filter applied as age greater than 21 and Groupby age with a display of count:

```jason
df.groupBy("age").count().show()
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic36.png?raw=true)

<b>Using Normal SQL</b>

* Execute commands to fetch the details on console using normal SQL and print the same on console:

```jason
import sqlContext.implicits._
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic37.png?raw=true)

```jason
case class Person(name: String, age: Int)
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic38.png?raw=true)

```jason
val people = sc.textFile("${HDFS}hadoop-learning-labs-people.txt ").map(_.split(",")).map(p  => Person(p(0), p(1).trim.toInt)).toDF()
```
Replace the hdfs path with the path obtained with Echo command in Step 1.


![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic39.png?raw=true)

```jason
people.registerTempTable("people")
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic40.png?raw=true)

```jason
val teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")
```


![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic41.png?raw=true)

```jason
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HADOOP 101 WITH BASIC TERMINAL HANDS-ON EXERCISES/assets/images/HadoopBasic42.png?raw=true)

Steps Performed:

1. Create an SQL context
2. Create a dataframe using the SQL context and data from the file existing in HDFS – “hadoop-learning-labs-people.json” 
  (DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a    relational database)
3. Execute command to display the contents of dataframe.
4. Execute command to select and display the name and age with the filter applied as age greater than 21 and Groupby age.
5. Execute command to fetch the details on console using normal SQL and print the same on console.

	
## LESSONS LEARNT :

1. How to use Apache Spark-shell to interact with Spark cluster. 

## NEXT?

1. How to use Hive command to interact with Hive cluster
