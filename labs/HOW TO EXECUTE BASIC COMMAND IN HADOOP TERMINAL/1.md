# <center>HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL</center>

Executed on : Hadoop Terminal

## OVERVIEW

This learning lab can be used as a guide to get a high level understanding of Apache Hadoop. The prime focus would be to,

1. Get a basic understanding of Hadoop infrastructure.

2. Learn how to execute HDFS command. 

3.	Learn how to use Apache Spark-shell to interact with Spark cluster.

4.	Learn how to use Hive command to interact with Hive cluster

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Obtain access to the Data Learning Platform by creating an account at https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology (Detailed instructions given below)

3. User should have a basic exposure to Core Java, database concepts and any of the Linux operating system flavors.


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform)

2. To get familiarized with Apache Hadoop.

3. To get familiarized with the terminal usage for executing HDFS commands.

## TERMINOLOGIES USED

### HADOOP - AN INTRODUCTION ###

What is Hadoop?

Hadoop is an open-source framework that allows to store and process big data in a distributed environment across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Hadoop splits files into large blocks and distributes them across nodes in a cluster. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework.

There are two fundamental things about Hadoop :

1. File Storage (known as Hadoop Distributed File System (HDFS)
2. Data Processing (known as MapReduce)

Hadoop Evolution Overview

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/Hadoop.png?raw=true)

Hadoop Architecture

(https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/Hadoop Architecture.png?raw=true)| width=12

<img src="https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/Hadoop Architecture.png" data-canonical-src="https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/Hadoop Architecture.png" width="600" height="300" />

Hadoop framework includes following four modules:

1. <b>Hadoop Common:</b> These are Java libraries and utilities required by other Hadoop modules. These libraries provides filesystem and OS level abstractions and contains the necessary Java files and scripts required to start Hadoop.

2. <b>Hadoop YARN:</b> This is a framework for job scheduling and cluster resource management.

3. <b>Hadoop Distributed File System (HDFS™):</b> A distributed file system that provides high-throughput access to application data.
4. <b>Hadoop MapReduce:</b> This is YARN-based system for parallel processing of large data sets.


For more details, please refer:

https://en.wikipedia.org/wiki/Apache_Hadoop

http://hadoop.apache.org/


### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is a Big-Data learning platform connected with an easy to learn UI to help you break into the world of Big-Data development.  Backed with Docker, best-in-class open source Big-Data tools and Cisco API’s, Big-Data Scientists and Developers can leverage DLP to easily build solutions, visualize data and turn POC’s for production applications. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/


## PROCESS OVERVIEW 

In this learning lab, we will focus on three different exercises which are as follows:

1.	How to execute HDFS command. 

2.	How to use Apache Spark-shell to interact with Spark cluster.

3.	How to use Hive command to interact with Hive cluster


### HOW TO EXECUTE HDFS COMMAND ###

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Process1.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the program.

###### Step 1: 

Login to DLP (Data Learning Platform)- https://devnetsandbox.cisco.com/RM/Diagram/Index/d8b9476d-cb6f-4b44-a236-b8ebdb3d3ef9?diagramType=Topology </br>. 

Give the login credentials if the account is already created, else, click on “Register now” button and create a new login profile. The screen shown on click of url is as follows:
If login credentials exist, login by providing the user id and password:
  
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Login Page_1.jpeg?raw=true)

If accessing for the first time, please register and create a new profile:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Register.jpeg?raw=true)


###### Step 2: 

After logging in, user will be directed to the sandbox UI as shown in the screen below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 3:

User needs to click on the Reserve button shown in sandbox page as shown in the screenshot below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Sandbox Lab.jpeg?raw=true)

###### Step 4:

On click of reserve button, a pop up window is shown with the details as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/Reserve_page1.jpeg?raw=true)

###### Step 5:

On click of Reserve button on the pop up window, user would receive an email on registered email ID with the login credentials. The email format would be as shown below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/email_2.jpeg?raw=true)

###### Step 6:

User needs to click on the link given in the email. On click of the link, the following page would be shown to the user.

Supply the credentials recieved with the email and click on “Login”.

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/dlp login page.jpeg?raw=true)

###### Step 7:

On login, user would be directed to the DevNet DLP (Data Learning Platform)dashboard page as shown in the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/word-count-using-spark/assets/images/DevNetLanding.jpeg?raw=true)

###### Step 8:

From Learning Labs pane, select the learning lab "An Introduction to Hadoop EcoSystem" and click on "Start" button as shown in screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic1.png?raw=true)

###### Step 9:

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic2.png?raw=true)

###### Step 9:

Points to Note:

* The task in Tools and Microservices - Others should be in running status and the colour of the icon should be Green for launch button to be enabled. Please refer screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic4.png?raw=true)

* If the task is in killed status then click on it to start again. Please refer screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic3.png?raw=true)

###### Step 11:

On click of launch button, the user will be navigated to hadoop terminal which would open in a separate tab as shown in screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic5.png?raw=true)

********TBD*********** Provide login id and password on the terminal prompt as shown in screenshot below: (Login ID : root / Password : cisco)

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic10.png?raw=true)

###### Step 12:

Execute some of the basic commands in hadoop terminal. 

Please find each command snippet with the output. Copy paste each of the commands given below to the terminal and view the output obtained.

<b>Create a directory in HDFS:</b>


* The input command is as follows:
```jason
hdfs dfs -mkdir ${HDFS}learning-labs/
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic11.png?raw=true)

* The output command is as follows:

```jason
hdfs dfs -mkdir ${HDFS}learning-labs/
```

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic12.png?raw=true)

* Output obtained on successful execution is as shown in screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic13.png?raw=true)

Note: HDFS – Environment Parameter and Learning-labs – directory name






<b>Upload a file to HDFS from local desktop:</b>

* The input command is as follows:
```jason
echo “hello, hadoop-learning-labs” >> hadoop-learning-labs.txt
```
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic14.png?raw=true)
```jason
hdfs dfs –put hadoop-learning-labs.txt ${HDFS}learning-labs/
```
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic15.png?raw=true)


Steps Performed:
1. A file is created in the local system with the content as hello, hadoop-learning-labs.
2. Upload file via hdfs dfs -put or hdfs dfs -copyFromLocal command from local file to HDFS.

* The output command is as follows:

```jason
hdfs dfs –ls ${HDFS}learning-labs/
```
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic17.png?raw=true)

* Output obtained on successful execution is as shown in screenshot below:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic16.png?raw=true)






![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic7.png?raw=true)

The command is as follows:
```jason

```

The output would be as shown below:

* Download a file from HDFS to local desktop:

![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic8.png?raw=true)

The command is as follows:
```jason

```

The output would be as shown below:


## LESSONS LEARNT :

1. A 

## <center>Congratulations! You have successfully completed the Learning Lab!
