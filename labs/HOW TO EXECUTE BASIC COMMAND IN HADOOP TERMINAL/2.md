
### HOW TO USE APACHE SPARK-SHELL TO INTERACT WITH SPARK CLUSTER###

###### Step 1:
From Hadoop Terminal, use echo command to find the path as shown below:
```jason
echo $HDFS
```
Output of the command is as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic29.png?raw=true)

Note: The path obtained needs to be used in commands shown below to replace {$HDFS}.


###### Step 2:
Spark shell interactive environment needs to be launched from terminal.Please follow the steps given below. User could launch the Spark shell interactive environment either in local mode or in cluster mode but not in both of them.

To launch in Local Mode, please follow the command given below:

```jason
spark-shell
```

To launch in cluster Mode, please follow the command given below:

```jason
spark-shell --master yarn --deploy-mode client
```

###### Step 4:

Execute word count transformation program with basic spark code as follows. The commands mentioned below needs to be executed in sequential order.

```jason
val inputfile = sc.textFile("${HDFS}hadoop-learning-labs-people.txt") 
```
Replace the hdfs path with the path obtained with Echo command in Step 1.

Output of the command is as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic25.png?raw=true)

```jason
val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_);
```
Output of the command is as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic26.png?raw=true)
```jason
counts.cache()
```
Output of the command is as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic27.png?raw=true)
```jason
counts.toArray().foreach(println)
```
Output of the command is as shown below:
![alt-tag](https://github.com/lpalamth/data-dev-learning-labs/blob/master/labs/HOW TO EXECUTE BASIC COMMAND IN HADOOP TERMINAL/assets/images/HadoopBasic28.png?raw=true)


Steps Performed:

1. 

###### Step 5:

Use SQL to do analytics in Spark shell, there are two different ways. 

1. Using Spark SQL
2. Using Normal SQL

The commands mentioned below needs to be executed in sequential order. 

<b>Using Spark SQL</b>

```jason
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
```

```jason
val df = sqlContext.jsonFile("${HDFS}hadoop-learning-labs-people.json ")
```


```jason
df.show()
```

```jason
df.select("name").show()
```

```jason
df.select(df("name"), df("age") + 1).show()
```

```jason
df.filter(df("age") > 21).show()
```

```jason
df.groupBy("age").count().show()
```

<b>Using Normal SQL</b>

```jason
import sqlContext.implicits._
	case class Person(name: String, age: Int)
```

```jason
val people = sc.textFile("${HDFS}hadoop-learning-labs-people.txt ").map(_.split(",")).map(p  => Person(p(0), p(1).trim.toInt)).toDF()
```

```jason
people.registerTempTable("people")
```

```jason
val teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 13 AND age <= 19")
```
	
```jason
teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
```

	
